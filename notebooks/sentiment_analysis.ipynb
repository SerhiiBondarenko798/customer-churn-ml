{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment Analysis for Customer Reviews\n",
        "\n"
      ],
      "metadata": {
        "id": "RR5N0ag2dKfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary libraries"
      ],
      "metadata": {
        "id": "VBxfTRstfUs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "import warnings\n",
        "from collections import OrderedDict\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "t-3XCMD1RqJa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download necessary NLTK resources"
      ],
      "metadata": {
        "id": "qwny6SWufrq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T8EoX8tRuJh",
        "outputId": "0972ae1c-db1c-4c91-db34-209862741df3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize NLP and sentiment analysis tools"
      ],
      "metadata": {
        "id": "FVjebFFVhkCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sid = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "nk6_QPbJRwug"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to map POS tags to WordNet format for lemmatization"
      ],
      "metadata": {
        "id": "_igg6lN4hv7-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dfu9XW9NwKEM"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(pos_tag):\n",
        "    if pos_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif pos_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif pos_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif pos_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to clean the text (lowercase, remove punctuation, stopwords, lemmatization, etc.)"
      ],
      "metadata": {
        "id": "Iq52Zsk_hzuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = [word.strip(string.punctuation) for word in text.split(\" \")]  # Tokenize and remove punctuation\n",
        "    text = [word for word in text if not any(c.isdigit() for c in word)]  # Remove words with numbers\n",
        "    stop = stopwords.words('english')  # Define stopwords\n",
        "    text = [x for x in text if x not in stop]  # Remove stopwords\n",
        "    pos_tags = pos_tag(text)  # POS tagging\n",
        "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]  # Lemmatization\n",
        "    text = [t for t in text if len(t) > 1]  # Remove words with only one letter\n",
        "    return \" \".join(text)"
      ],
      "metadata": {
        "id": "TjvhEXKAR3ES"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import sentiment data"
      ],
      "metadata": {
        "id": "Ch_tUSX_h3vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgXGHliCS7-v",
        "outputId": "2006835e-5eca-4edc-b812-e3eb83dff500"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and preprocess sentiment data"
      ],
      "metadata": {
        "id": "r6jp4Qebh6T0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df = pd.read_json('/content/drive/MyDrive/churn_prediction/data/reviews/Automotive_for_train.json')\n",
        "sentiment_df = sentiment_df.dropna().replace(to_replace='None', value=np.nan).dropna()\n",
        "sentiment_df = sentiment_df.rename(columns={\"score\": \"is_neg\", \"content\": \"review\"})\n",
        "sentiment_df = sentiment_df.sample(frac=0.1, random_state=14)\n",
        "sentiment_df['is_neg'] = sentiment_df['is_neg'].apply(lambda x: 1 if int(x) < 3 else 0)"
      ],
      "metadata": {
        "id": "Jn15F49oQ5Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean review and summary text"
      ],
      "metadata": {
        "id": "zOovOaF1h8Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df[\"review_clean\"] = sentiment_df[\"review\"].apply(lambda x: clean_text(x))\n",
        "sentiment_df[\"summary_clean\"] = sentiment_df[\"summary\"].apply(lambda x: clean_text(x))"
      ],
      "metadata": {
        "id": "Bjsb9ulLQ5Ma"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply VADER sentiment analysis to reviews and summaries"
      ],
      "metadata": {
        "id": "vrqjiEsgh9td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentiment_df[\"sentiments\"] = sentiment_df[\"review\"].apply(lambda x: sid.polarity_scores(x))\n",
        "sentiment_df = pd.concat([sentiment_df.drop(['sentiments'], axis=1), sentiment_df['sentiments'].apply(pd.Series)], axis=1)\n",
        "sentiment_df = sentiment_df.rename(columns={\"neg\": \"neg_rw\", \"neu\": \"neu_rw\", \"pos\": \"pos_rw\", \"compound\": \"compound_rw\"})\n",
        "\n",
        "sentiment_df[\"sentiments_sm\"] = sentiment_df[\"summary\"].apply(lambda x: sid.polarity_scores(x))\n",
        "sentiment_df = pd.concat([sentiment_df.drop(['sentiments_sm'], axis=1), sentiment_df['sentiments_sm'].apply(pd.Series)], axis=1)\n",
        "sentiment_df = sentiment_df.rename(columns={\"neg\": \"neg_sm\", \"neu\": \"neu_sm\", \"pos\": \"pos_sm\", \"compound\": \"compound_sm\"})"
      ],
      "metadata": {
        "id": "QWxWCJmGQ5H8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add additional features (number of characters and words)"
      ],
      "metadata": {
        "id": "5R0DioBEh_Rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df[\"nb_chars\"] = sentiment_df[\"review\"].apply(lambda x: len(x))\n",
        "sentiment_df[\"nb_chars_sm\"] = sentiment_df[\"summary\"].apply(lambda x: len(x))\n",
        "sentiment_df[\"nb_words\"] = sentiment_df[\"review\"].apply(lambda x: len(x.split(\" \")))\n",
        "sentiment_df[\"nb_words_sm\"] = sentiment_df[\"summary\"].apply(lambda x: len(x.split(\" \")))"
      ],
      "metadata": {
        "id": "FJ_IBwL_Q40Q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a Doc2Vec model and generate document vectors for reviews and summaries"
      ],
      "metadata": {
        "id": "nzRo4QO-iAxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = sentiment_df[\"review_clean\"].apply(lambda x: x.split(\" \"))\n",
        "summaries = sentiment_df[\"summary_clean\"].apply(lambda x: x.split(\" \"))\n",
        "all_texts = reviews.tolist() + summaries.tolist()\n",
        "documents = [TaggedDocument(words=doc, tags=[i]) for i, doc in enumerate(all_texts)]\n",
        "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "daRF2jGcQ4v1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Doc2Vec vectors for reviews and summaries"
      ],
      "metadata": {
        "id": "3UcUKWsBiCVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc2vec_df = reviews.apply(lambda x: model.infer_vector(x)).apply(pd.Series)\n",
        "doc2vec_df.columns = [\"doc2vec_rw_vector_\" + str(x) for x in doc2vec_df.columns]\n",
        "sentiment_df = pd.concat([sentiment_df, doc2vec_df], axis=1)\n",
        "\n",
        "doc2vec_df_sm = summaries.apply(lambda x: model.infer_vector(x)).apply(pd.Series)\n",
        "doc2vec_df_sm.columns = [\"doc2vec_sm_vector_\" + str(x) for x in doc2vec_df_sm.columns]\n",
        "sentiment_df = pd.concat([sentiment_df, doc2vec_df_sm], axis=1)"
      ],
      "metadata": {
        "id": "KdFEWndRQ4ra"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine cleaned review and summary for TF-IDF vectorization"
      ],
      "metadata": {
        "id": "NCYIo_nNiDzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merge_clean = sentiment_df[[\"review_clean\", \"summary_clean\"]].apply(lambda x: \" \".join(x), axis=1)\n",
        "merge_clean = merge_clean.apply(lambda x: list(OrderedDict.fromkeys(x.split(\" \"))))\n",
        "merge_clean = merge_clean.apply(lambda x: \" \".join(x))"
      ],
      "metadata": {
        "id": "LTShubJhQ6dU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and apply TF-IDF vectorizer"
      ],
      "metadata": {
        "id": "-YUOKx6xiGby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_model = TfidfVectorizer(min_df=10)\n",
        "tfidf_model.fit(merge_clean)\n",
        "\n",
        "tfidf_result = tfidf_model.transform(merge_clean).toarray()\n",
        "tfidf_df = pd.DataFrame(tfidf_result, columns=tfidf_model.get_feature_names_out())\n",
        "tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n",
        "tfidf_df.index = sentiment_df.index\n",
        "sentiment_df = pd.concat([sentiment_df, tfidf_df], axis=1)"
      ],
      "metadata": {
        "id": "0WncLvtoQ4mH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove duplicate columns"
      ],
      "metadata": {
        "id": "hICWc-lXiIEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df = sentiment_df.loc[:, ~sentiment_df.columns.duplicated()].copy()"
      ],
      "metadata": {
        "id": "BUrMNbpnQ4jN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-test split for the sentiment prediction model"
      ],
      "metadata": {
        "id": "bir0RKYKiJnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = \"is_neg\"\n",
        "ignore_cols = [label, \"review\", \"review_clean\", \"summary\", \"summary_clean\"]\n",
        "features = [c for c in sentiment_df.columns if c not in ignore_cols]\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentiment_df[features], sentiment_df[label], test_size=0.20, random_state=42)\n"
      ],
      "metadata": {
        "id": "ePsIvE41RPlC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train XGBoost model for sentiment classification"
      ],
      "metadata": {
        "id": "FGKEr64iiLON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(objective=\"binary:logistic\", n_jobs=-1, n_estimators=1000, max_depth=10)\n",
        "xgb.fit(X_train, y_train)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fsw2YBM-RPfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save trained models"
      ],
      "metadata": {
        "id": "14oiQr62iRtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(tfidf_model, 'sentimenta_tfidf_model')\n",
        "joblib.dump(xgb, 'sentimenta_xgb_model.pkl')\n",
        "model.save('sentimenta_doc2vec_model')"
      ],
      "metadata": {
        "id": "clnVJQT_RPch"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for sentiment analysis on reviews and summary"
      ],
      "metadata": {
        "id": "VS2-BX63iUJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sentiment_feedback(sentiment_df, doc2vec_model, tfidf_vectorizer, xgb_model):\n",
        "    \"\"\"\n",
        "    Processes customer reviews and summary to extract sentiment features and\n",
        "    vector representations using TF-IDF and Doc2Vec, and predicts sentiment scores.\n",
        "\n",
        "    Parameters:\n",
        "    - sentiment_df: DataFrame containing review and summary text.\n",
        "    - doc2vec_model: Trained Doc2Vec model for vector representation of text.\n",
        "    - tfidf_vectorizer: Trained TF-IDF vectorizer.\n",
        "    - xgb_model: Trained XGBoost model for sentiment prediction.\n",
        "\n",
        "    Returns:\n",
        "    - output_df: DataFrame containing predicted sentiment and sentiment score.\n",
        "    \"\"\"\n",
        "    # Clean the text data in the 'review' and 'summary' columns\n",
        "    sentiment_df[\"review_clean\"] = sentiment_df[\"review\"].apply(lambda x: clean_text(x))\n",
        "    sentiment_df[\"summary_clean\"] = sentiment_df[\"summary\"].apply(lambda x: clean_text(x))\n",
        "\n",
        "    # Apply VADER sentiment analysis on reviews and summaries\n",
        "    sentiment_df[\"sentiments\"] = sentiment_df[\"review\"].apply(lambda x: sid.polarity_scores(x))\n",
        "    sentiment_df = pd.concat([sentiment_df.drop(['sentiments'], axis=1), sentiment_df['sentiments'].apply(pd.Series)], axis=1)\n",
        "    sentiment_df = sentiment_df.rename(columns={\"neg\": \"neg_rw\", \"neu\": \"neu_rw\", \"pos\": \"pos_rw\", \"compound\": \"compound_rw\"})\n",
        "\n",
        "    sentiment_df[\"sentiments_sm\"] = sentiment_df[\"summary\"].apply(lambda x: sid.polarity_scores(x))\n",
        "    sentiment_df = pd.concat([sentiment_df.drop(['sentiments_sm'], axis=1), sentiment_df['sentiments_sm'].apply(pd.Series)], axis=1)\n",
        "    sentiment_df = sentiment_df.rename(columns={\"neg\": \"neg_sm\", \"neu\": \"neu_sm\", \"pos\": \"pos_sm\", \"compound\": \"compound_sm\"})\n",
        "\n",
        "    # Add features for number of characters and words in the review and summary\n",
        "    sentiment_df[\"nb_chars\"] = sentiment_df[\"review\"].apply(lambda x: len(x))\n",
        "    sentiment_df[\"nb_chars_sm\"] = sentiment_df[\"summary\"].apply(lambda x: len(x))\n",
        "    sentiment_df[\"nb_words\"] = sentiment_df[\"review\"].apply(lambda x: len(x.split(\" \")))\n",
        "    sentiment_df[\"nb_words_sm\"] = sentiment_df[\"summary\"].apply(lambda x: len(x.split(\" \")))\n",
        "\n",
        "    # Generate Doc2Vec vectors for review and summary text\n",
        "    doc2vec_df = sentiment_df[\"review_clean\"].apply(lambda x: doc2vec_model.infer_vector(x.split(\" \"))).apply(pd.Series)\n",
        "    doc2vec_df.columns = [\"doc2vec_rw_vector_\" + str(x) for x in doc2vec_df.columns]\n",
        "    sentiment_df = pd.concat([sentiment_df, doc2vec_df], axis=1)\n",
        "\n",
        "    doc2vec_df_sm = sentiment_df[\"summary_clean\"].apply(lambda x: doc2vec_model.infer_vector(x.split(\" \"))).apply(pd.Series)\n",
        "    doc2vec_df_sm.columns = [\"doc2vec_sm_vector_\" + str(x) for x in doc2vec_df_sm.columns]\n",
        "    sentiment_df = pd.concat([sentiment_df, doc2vec_df_sm], axis=1)\n",
        "\n",
        "    # Combine cleaned review and summary for TF-IDF vectorization\n",
        "    merge_clean = sentiment_df[[\"review_clean\", \"summary_clean\"]].apply(lambda x: \" \".join(x), axis=1)\n",
        "    merge_clean = merge_clean.apply(lambda x: list(OrderedDict.fromkeys(x.split(\" \"))))\n",
        "    merge_clean = merge_clean.apply(lambda x: \" \".join(x))\n",
        "\n",
        "    # Apply TF-IDF vectorizer\n",
        "    tfidf_result = tfidf_vectorizer.transform(merge_clean).toarray()\n",
        "    tfidf_df = pd.DataFrame(tfidf_result, columns=tfidf_vectorizer.get_feature_names_out())\n",
        "    tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n",
        "    tfidf_df.index = sentiment_df.index\n",
        "    sentiment_df = pd.concat([sentiment_df, tfidf_df], axis=1)\n",
        "\n",
        "    # Remove duplicate columns\n",
        "    sentiment_df = sentiment_df.loc[:, ~sentiment_df.columns.duplicated()].copy()\n",
        "\n",
        "    # Define features for sentiment prediction\n",
        "    ignore_cols = [\"review\", \"review_clean\", \"summary\", \"summary_clean\"]\n",
        "    features = [c for c in sentiment_df.columns if c not in ignore_cols]\n",
        "\n",
        "    # Make predictions with XGBoost model\n",
        "    X_test = sentiment_df[features]\n",
        "    y_pred_list = xgb_model.predict_proba(X_test)\n",
        "    y_pred = [np.max(x) for x in y_pred_list]\n",
        "    y_pred_f = xgb_model.predict(X_test)\n",
        "    y_pred_f = list(y_pred_f)\n",
        "\n",
        "    # Prepare output DataFrame\n",
        "    output_df = pd.DataFrame(list(zip(y_pred_f, y_pred)), columns=['sentiment', 'sent_score'])\n",
        "\n",
        "    return output_df"
      ],
      "metadata": {
        "id": "sXE1nLXYRhUh"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}